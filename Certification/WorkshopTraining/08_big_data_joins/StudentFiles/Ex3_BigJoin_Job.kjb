<?xml version="1.0" encoding="UTF-8"?>
<job>
  <name>Ex3_BigJoin_Job</name>
    <description/>
    <extended_description/>
    <job_version/>
    <job_status>0</job_status>
  <directory>&#x2f;</directory>
  <created_user>-</created_user>
  <created_date>2015&#x2f;05&#x2f;15 15&#x3a;41&#x3a;50.501</created_date>
  <modified_user>-</modified_user>
  <modified_date>2015&#x2f;05&#x2f;15 15&#x3a;41&#x3a;50.501</modified_date>
    <parameters>
    </parameters>
  <connection>
    <name>AgileBI</name>
    <server>localhost</server>
    <type>MONETDB</type>
    <access>Native</access>
    <database>pentaho-instaview</database>
    <port>50006</port>
    <username>monetdb</username>
    <password>Encrypted 2be98afc86aa7f2e4cb14a17edb86abd8</password>
    <servername/>
    <data_tablespace/>
    <index_tablespace/>
    <read_only>true</read_only>
    <attributes>
      <attribute><code>EXTRA_OPTION_INFOBRIGHT.characterEncoding</code><attribute>UTF-8</attribute></attribute>
      <attribute><code>EXTRA_OPTION_MYSQL.defaultFetchSize</code><attribute>500</attribute></attribute>
      <attribute><code>EXTRA_OPTION_MYSQL.useCursorFetch</code><attribute>true</attribute></attribute>
      <attribute><code>PORT_NUMBER</code><attribute>50006</attribute></attribute>
      <attribute><code>PRESERVE_RESERVED_WORD_CASE</code><attribute>Y</attribute></attribute>
      <attribute><code>SUPPORTS_BOOLEAN_DATA_TYPE</code><attribute>Y</attribute></attribute>
      <attribute><code>SUPPORTS_TIMESTAMP_DATA_TYPE</code><attribute>Y</attribute></attribute>
    </attributes>
  </connection>
  <connection>
    <name>Hive</name>
    <server>localhost</server>
    <type>HIVE2</type>
    <access>Native</access>
    <database>default</database>
    <port>9988</port>
    <username/>
    <password>Encrypted </password>
    <servername/>
    <data_tablespace/>
    <index_tablespace/>
    <attributes>
      <attribute><code>FORCE_IDENTIFIERS_TO_LOWERCASE</code><attribute>N</attribute></attribute>
      <attribute><code>FORCE_IDENTIFIERS_TO_UPPERCASE</code><attribute>N</attribute></attribute>
      <attribute><code>IS_CLUSTERED</code><attribute>N</attribute></attribute>
      <attribute><code>PORT_NUMBER</code><attribute>9988</attribute></attribute>
      <attribute><code>PRESERVE_RESERVED_WORD_CASE</code><attribute>N</attribute></attribute>
      <attribute><code>QUOTE_ALL_FIELDS</code><attribute>N</attribute></attribute>
      <attribute><code>SUPPORTS_BOOLEAN_DATA_TYPE</code><attribute>Y</attribute></attribute>
      <attribute><code>SUPPORTS_TIMESTAMP_DATA_TYPE</code><attribute>Y</attribute></attribute>
      <attribute><code>USE_POOLING</code><attribute>N</attribute></attribute>
    </attributes>
  </connection>
  <connection>
    <name>Impala</name>
    <server>localhost</server>
    <type>IMPALA</type>
    <access>Native</access>
    <database>default</database>
    <port>21050</port>
    <username>zeus</username>
    <password>Encrypted 2be98afc86aa7f2e4cb79ce10c497bac9</password>
    <servername/>
    <data_tablespace/>
    <index_tablespace/>
    <attributes>
      <attribute><code>FORCE_IDENTIFIERS_TO_LOWERCASE</code><attribute>N</attribute></attribute>
      <attribute><code>FORCE_IDENTIFIERS_TO_UPPERCASE</code><attribute>N</attribute></attribute>
      <attribute><code>IS_CLUSTERED</code><attribute>N</attribute></attribute>
      <attribute><code>PORT_NUMBER</code><attribute>21050</attribute></attribute>
      <attribute><code>PRESERVE_RESERVED_WORD_CASE</code><attribute>N</attribute></attribute>
      <attribute><code>QUOTE_ALL_FIELDS</code><attribute>N</attribute></attribute>
      <attribute><code>SUPPORTS_BOOLEAN_DATA_TYPE</code><attribute>Y</attribute></attribute>
      <attribute><code>SUPPORTS_TIMESTAMP_DATA_TYPE</code><attribute>Y</attribute></attribute>
      <attribute><code>USE_POOLING</code><attribute>N</attribute></attribute>
    </attributes>
  </connection>
  <connection>
    <name>ImpalaJoin</name>
    <server>localhost</server>
    <type>IMPALA</type>
    <access>Native</access>
    <database>default</database>
    <port>21050</port>
    <username/>
    <password>Encrypted </password>
    <servername/>
    <data_tablespace/>
    <index_tablespace/>
    <attributes>
      <attribute><code>FORCE_IDENTIFIERS_TO_LOWERCASE</code><attribute>N</attribute></attribute>
      <attribute><code>FORCE_IDENTIFIERS_TO_UPPERCASE</code><attribute>N</attribute></attribute>
      <attribute><code>IS_CLUSTERED</code><attribute>N</attribute></attribute>
      <attribute><code>PORT_NUMBER</code><attribute>21050</attribute></attribute>
      <attribute><code>PRESERVE_RESERVED_WORD_CASE</code><attribute>N</attribute></attribute>
      <attribute><code>QUOTE_ALL_FIELDS</code><attribute>N</attribute></attribute>
      <attribute><code>SUPPORTS_BOOLEAN_DATA_TYPE</code><attribute>Y</attribute></attribute>
      <attribute><code>SUPPORTS_TIMESTAMP_DATA_TYPE</code><attribute>Y</attribute></attribute>
      <attribute><code>USE_POOLING</code><attribute>N</attribute></attribute>
    </attributes>
  </connection>
  <connection>
    <name>local-mysql-test</name>
    <server>localhost</server>
    <type>MYSQL</type>
    <access>Native</access>
    <database>big_wireless</database>
    <port>3306</port>
    <username>root</username>
    <password>Encrypted 2eeabc1bc0bcfc2c59b0bab25df9eaac9</password>
    <servername/>
    <data_tablespace/>
    <index_tablespace/>
    <attributes>
      <attribute><code>EXTRA_OPTION_MYSQL.defaultFetchSize</code><attribute>500</attribute></attribute>
      <attribute><code>EXTRA_OPTION_MYSQL.useCursorFetch</code><attribute>true</attribute></attribute>
      <attribute><code>FORCE_IDENTIFIERS_TO_LOWERCASE</code><attribute>N</attribute></attribute>
      <attribute><code>FORCE_IDENTIFIERS_TO_UPPERCASE</code><attribute>N</attribute></attribute>
      <attribute><code>IS_CLUSTERED</code><attribute>N</attribute></attribute>
      <attribute><code>PORT_NUMBER</code><attribute>3306</attribute></attribute>
      <attribute><code>PRESERVE_RESERVED_WORD_CASE</code><attribute>N</attribute></attribute>
      <attribute><code>QUOTE_ALL_FIELDS</code><attribute>N</attribute></attribute>
      <attribute><code>STREAM_RESULTS</code><attribute>Y</attribute></attribute>
      <attribute><code>SUPPORTS_BOOLEAN_DATA_TYPE</code><attribute>Y</attribute></attribute>
      <attribute><code>SUPPORTS_TIMESTAMP_DATA_TYPE</code><attribute>Y</attribute></attribute>
      <attribute><code>USE_POOLING</code><attribute>N</attribute></attribute>
    </attributes>
  </connection>
  <connection>
    <name>Vertica</name>
    <server>127.0.0.1</server>
    <type>VERTICA5</type>
    <access>Native</access>
    <database>bigdata</database>
    <port>5443</port>
    <username>dbadmin</username>
    <password>Encrypted 2be98afc86aa7f2e4bb18bd63c99dbdde</password>
    <servername/>
    <data_tablespace/>
    <index_tablespace/>
    <attributes>
      <attribute><code>FORCE_IDENTIFIERS_TO_LOWERCASE</code><attribute>N</attribute></attribute>
      <attribute><code>FORCE_IDENTIFIERS_TO_UPPERCASE</code><attribute>N</attribute></attribute>
      <attribute><code>IS_CLUSTERED</code><attribute>N</attribute></attribute>
      <attribute><code>PORT_NUMBER</code><attribute>5443</attribute></attribute>
      <attribute><code>PRESERVE_RESERVED_WORD_CASE</code><attribute>N</attribute></attribute>
      <attribute><code>QUOTE_ALL_FIELDS</code><attribute>N</attribute></attribute>
      <attribute><code>SUPPORTS_BOOLEAN_DATA_TYPE</code><attribute>Y</attribute></attribute>
      <attribute><code>SUPPORTS_TIMESTAMP_DATA_TYPE</code><attribute>Y</attribute></attribute>
      <attribute><code>USE_POOLING</code><attribute>N</attribute></attribute>
    </attributes>
  </connection>
    <slaveservers>
         <slaveserver><name>local_cart</name><hostname>localhost</hostname><port>9080</port><webAppName>pentaho-di</webAppName><username>admin</username><password>Encrypted 2be98afc86aa7f2e4bb18bd63c99dbdde</password><proxy_hostname/><proxy_port/><non_proxy_hosts/><master>N</master><sslMode>N</sslMode></slaveserver>
    </slaveservers>
<job-log-table><connection/>
<schema/>
<table/>
<size_limit_lines/>
<interval/>
<timeout_days/>
<field><id>ID_JOB</id><enabled>Y</enabled><name>ID_JOB</name></field><field><id>CHANNEL_ID</id><enabled>Y</enabled><name>CHANNEL_ID</name></field><field><id>JOBNAME</id><enabled>Y</enabled><name>JOBNAME</name></field><field><id>STATUS</id><enabled>Y</enabled><name>STATUS</name></field><field><id>LINES_READ</id><enabled>Y</enabled><name>LINES_READ</name></field><field><id>LINES_WRITTEN</id><enabled>Y</enabled><name>LINES_WRITTEN</name></field><field><id>LINES_UPDATED</id><enabled>Y</enabled><name>LINES_UPDATED</name></field><field><id>LINES_INPUT</id><enabled>Y</enabled><name>LINES_INPUT</name></field><field><id>LINES_OUTPUT</id><enabled>Y</enabled><name>LINES_OUTPUT</name></field><field><id>LINES_REJECTED</id><enabled>Y</enabled><name>LINES_REJECTED</name></field><field><id>ERRORS</id><enabled>Y</enabled><name>ERRORS</name></field><field><id>STARTDATE</id><enabled>Y</enabled><name>STARTDATE</name></field><field><id>ENDDATE</id><enabled>Y</enabled><name>ENDDATE</name></field><field><id>LOGDATE</id><enabled>Y</enabled><name>LOGDATE</name></field><field><id>DEPDATE</id><enabled>Y</enabled><name>DEPDATE</name></field><field><id>REPLAYDATE</id><enabled>Y</enabled><name>REPLAYDATE</name></field><field><id>LOG_FIELD</id><enabled>Y</enabled><name>LOG_FIELD</name></field><field><id>EXECUTING_SERVER</id><enabled>N</enabled><name>EXECUTING_SERVER</name></field><field><id>EXECUTING_USER</id><enabled>N</enabled><name>EXECUTING_USER</name></field><field><id>START_JOB_ENTRY</id><enabled>N</enabled><name>START_JOB_ENTRY</name></field><field><id>CLIENT</id><enabled>N</enabled><name>CLIENT</name></field></job-log-table>
<jobentry-log-table><connection/>
<schema/>
<table/>
<timeout_days/>
<field><id>ID_BATCH</id><enabled>Y</enabled><name>ID_BATCH</name></field><field><id>CHANNEL_ID</id><enabled>Y</enabled><name>CHANNEL_ID</name></field><field><id>LOG_DATE</id><enabled>Y</enabled><name>LOG_DATE</name></field><field><id>JOBNAME</id><enabled>Y</enabled><name>TRANSNAME</name></field><field><id>JOBENTRYNAME</id><enabled>Y</enabled><name>STEPNAME</name></field><field><id>LINES_READ</id><enabled>Y</enabled><name>LINES_READ</name></field><field><id>LINES_WRITTEN</id><enabled>Y</enabled><name>LINES_WRITTEN</name></field><field><id>LINES_UPDATED</id><enabled>Y</enabled><name>LINES_UPDATED</name></field><field><id>LINES_INPUT</id><enabled>Y</enabled><name>LINES_INPUT</name></field><field><id>LINES_OUTPUT</id><enabled>Y</enabled><name>LINES_OUTPUT</name></field><field><id>LINES_REJECTED</id><enabled>Y</enabled><name>LINES_REJECTED</name></field><field><id>ERRORS</id><enabled>Y</enabled><name>ERRORS</name></field><field><id>RESULT</id><enabled>Y</enabled><name>RESULT</name></field><field><id>NR_RESULT_ROWS</id><enabled>Y</enabled><name>NR_RESULT_ROWS</name></field><field><id>NR_RESULT_FILES</id><enabled>Y</enabled><name>NR_RESULT_FILES</name></field><field><id>LOG_FIELD</id><enabled>N</enabled><name>LOG_FIELD</name></field><field><id>COPY_NR</id><enabled>N</enabled><name>COPY_NR</name></field></jobentry-log-table>
<channel-log-table><connection/>
<schema/>
<table/>
<timeout_days/>
<field><id>ID_BATCH</id><enabled>Y</enabled><name>ID_BATCH</name></field><field><id>CHANNEL_ID</id><enabled>Y</enabled><name>CHANNEL_ID</name></field><field><id>LOG_DATE</id><enabled>Y</enabled><name>LOG_DATE</name></field><field><id>LOGGING_OBJECT_TYPE</id><enabled>Y</enabled><name>LOGGING_OBJECT_TYPE</name></field><field><id>OBJECT_NAME</id><enabled>Y</enabled><name>OBJECT_NAME</name></field><field><id>OBJECT_COPY</id><enabled>Y</enabled><name>OBJECT_COPY</name></field><field><id>REPOSITORY_DIRECTORY</id><enabled>Y</enabled><name>REPOSITORY_DIRECTORY</name></field><field><id>FILENAME</id><enabled>Y</enabled><name>FILENAME</name></field><field><id>OBJECT_ID</id><enabled>Y</enabled><name>OBJECT_ID</name></field><field><id>OBJECT_REVISION</id><enabled>Y</enabled><name>OBJECT_REVISION</name></field><field><id>PARENT_CHANNEL_ID</id><enabled>Y</enabled><name>PARENT_CHANNEL_ID</name></field><field><id>ROOT_CHANNEL_ID</id><enabled>Y</enabled><name>ROOT_CHANNEL_ID</name></field></channel-log-table>
<checkpoint-log-table><connection/>
<schema/>
<table/>
<timeout_days/>
<max_nr_retries/>
<run_retry_period/>
<namespace_parameter/>
<save_parameters>Y</save_parameters>
<save_result_rows>Y</save_result_rows>
<save_result_files>Y</save_result_files>
<field><id>ID_JOB_RUN</id><enabled>Y</enabled><name>ID_JOB_RUN</name></field><field><id>ID_JOB</id><enabled>Y</enabled><name>ID_JOB</name></field><field><id>JOBNAME</id><enabled>Y</enabled><name>JOBNAME</name></field><field><id>NAMESPACE</id><enabled>Y</enabled><name>NAMESPACE</name></field><field><id>CHECKPOINT_NAME</id><enabled>Y</enabled><name>CHECKPOINT_NAME</name></field><field><id>CHECKPOINT_COPYNR</id><enabled>Y</enabled><name>CHECKPOINT_COPYNR</name></field><field><id>ATTEMPT_NR</id><enabled>Y</enabled><name>ATTEMPT_NR</name></field><field><id>JOB_RUN_START_DATE</id><enabled>Y</enabled><name>JOB_RUN_START_DATE</name></field><field><id>LOGDATE</id><enabled>Y</enabled><name>LOGDATE</name></field><field><id>RESULT_XML</id><enabled>Y</enabled><name>RESULT_XML</name></field><field><id>PARAMETER_XML</id><enabled>Y</enabled><name>PARAMETER_XML</name></field></checkpoint-log-table>
   <pass_batchid>N</pass_batchid>
   <shared_objects_file/>
  <entries>
    <entry>
      <name>START</name>
      <description/>
      <type>SPECIAL</type>
      <start>Y</start>
      <dummy>N</dummy>
      <repeat>N</repeat>
      <schedulerType>0</schedulerType>
      <intervalSeconds>0</intervalSeconds>
      <intervalMinutes>60</intervalMinutes>
      <hour>12</hour>
      <minutes>0</minutes>
      <weekDay>1</weekDay>
      <DayOfMonth>1</DayOfMonth>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>128</xloc>
      <yloc>64</yloc>
      </entry>
    <entry>
      <name>Success</name>
      <description/>
      <type>SUCCESS</type>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>928</xloc>
      <yloc>560</yloc>
      </entry>
    <entry>
      <name>Populate Tower Table</name>
      <description/>
      <type>SQL</type>
      <sql/>
      <useVariableSubstitution>T</useVariableSubstitution>
      <sqlfromfile>F</sqlfromfile>
      <sqlfilename/>
      <sendOneStatement>F</sendOneStatement>
      <connection>ImpalaJoin</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>128</xloc>
      <yloc>240</yloc>
      </entry>
    <entry>
      <name>Populate Call Data Table</name>
      <description/>
      <type>SQL</type>
      <sql/>
      <useVariableSubstitution>T</useVariableSubstitution>
      <sqlfromfile>F</sqlfromfile>
      <sqlfilename/>
      <sendOneStatement>F</sendOneStatement>
      <connection>ImpalaJoin</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>288</xloc>
      <yloc>240</yloc>
      </entry>
    <entry>
      <name>Create Table for Joned Data Set</name>
      <description/>
      <type>SQL</type>
      <sql/>
      <useVariableSubstitution>T</useVariableSubstitution>
      <sqlfromfile>F</sqlfromfile>
      <sqlfilename/>
      <sendOneStatement>F</sendOneStatement>
      <connection>ImpalaJoin</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>128</xloc>
      <yloc>560</yloc>
      </entry>
    <entry>
      <name>Pentaho MapReduce</name>
      <description/>
      <type>HadoopTransJobExecutorPlugin</type>
      <hadoop_job_name>Process Joined Data Set</hadoop_job_name>
      <map_trans_repo_dir/>
      <map_trans_repo_file/>
      <map_trans_repo_reference/>
      <map_trans>&#x2f;home&#x2f;zeus&#x2f;Desktop&#x2f;WorkshopTraining&#x2f;08_big_data_joins&#x2f;Solution&#x2f;Ex3_Mapper.ktr</map_trans>
      <combiner_trans_repo_dir/>
      <combiner_trans_repo_file/>
      <combiner_trans_repo_reference/>
      <combiner_trans/>
      <combiner_single_threaded>Y</combiner_single_threaded>
      <reduce_trans_repo_dir/>
      <reduce_trans_repo_file/>
      <reduce_trans_repo_reference/>
      <reduce_trans>&#x2f;home&#x2f;zeus&#x2f;Desktop&#x2f;WorkshopTraining&#x2f;08_big_data_joins&#x2f;Solution&#x2f;Ex3_Reducer.ktr</reduce_trans>
      <reduce_single_threaded>N</reduce_single_threaded>
      <map_input_step_name>MapReduce Input</map_input_step_name>
      <map_output_step_name>MapReduce Output</map_output_step_name>
      <combiner_input_step_name/>
      <combiner_output_step_name/>
      <reduce_input_step_name>MapReduce Input</reduce_input_step_name>
      <reduce_output_step_name>MapReduce Output</reduce_output_step_name>
      <blocking>Y</blocking>
      <logging_interval>5</logging_interval>
      <input_path>&#x24;&#x7b;app.hdfs.base.dir&#x7d;&#x24;&#x7b;app.hdfs.tables.dir&#x7d;&#x24;&#x7b;app.hdfs.tables.join&#x7d;</input_path>
      <input_format_class>org.apache.hadoop.mapred.TextInputFormat</input_format_class>
      <output_path>&#x24;&#x7b;app.hdfs.base.dir&#x7d;&#x2f;Ex3_Output</output_path>
      <clean_output_path>Y</clean_output_path>
      <suppress_output_map_key>N</suppress_output_map_key>
      <suppress_output_map_value>N</suppress_output_map_value>
      <suppress_output_key>Y</suppress_output_key>
      <suppress_output_value>N</suppress_output_value>
      <output_format_class>org.apache.hadoop.mapred.TextOutputFormat</output_format_class>
      <cluster_name>CDH 5.3</cluster_name>
      <hdfs_hostname>localhost</hdfs_hostname>
      <hdfs_port>8020</hdfs_port>
      <job_tracker_hostname>localhost</job_tracker_hostname>
      <job_tracker_port>8021</job_tracker_port>
      <num_map_tasks>1</num_map_tasks>
      <num_reduce_tasks/>
      <user_defined_list>
        <user_defined>
          <name>mapred.map.child.java.opts</name>
          <value>-Xmx512m</value>
        </user_defined>
        <user_defined>
          <name>mapred.reduce.child.java.opts</name>
          <value>-Xmx128m</value>
        </user_defined>
        <user_defined>
          <name>yarn.app.mapreduce.am.resource.mb</name>
          <value>512</value>
        </user_defined>
      </user_defined_list>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>544</xloc>
      <yloc>560</yloc>
      </entry>
    <entry>
      <name>Abort job</name>
      <description/>
      <type>ABORT</type>
      <message/>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>128</xloc>
      <yloc>368</yloc>
      </entry>
    <entry>
      <name>Populate Joined Table</name>
      <description/>
      <type>SQL</type>
      <sql/>
      <useVariableSubstitution>T</useVariableSubstitution>
      <sqlfromfile>F</sqlfromfile>
      <sqlfilename/>
      <sendOneStatement>F</sendOneStatement>
      <connection>ImpalaJoin</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>352</xloc>
      <yloc>560</yloc>
      </entry>
    <entry>
      <name>Init Vars</name>
      <description/>
      <type>SET_VARIABLES</type>
      <replacevars>Y</replacevars>
      <filename/>
      <file_variable_type>JVM</file_variable_type>
      <fields>
        <field>
          <variable_name>app.hadoop.hdfs.host</variable_name>
          <variable_value>localhost</variable_value>
          <variable_type>ROOT_JOB</variable_type>
        </field>
        <field>
          <variable_name>app.hadoop.hdfs.port</variable_name>
          <variable_value>8020</variable_value>
          <variable_type>ROOT_JOB</variable_type>
        </field>
        <field>
          <variable_name>app.hadoop.jobtracker.host</variable_name>
          <variable_value>localhost</variable_value>
          <variable_type>ROOT_JOB</variable_type>
        </field>
        <field>
          <variable_name>app.hadoop.jobtracker.port</variable_name>
          <variable_value>8021</variable_value>
          <variable_type>ROOT_JOB</variable_type>
        </field>
        <field>
          <variable_name>app.hdfs.base.dir</variable_name>
          <variable_value>&#x2f;join_callrecords</variable_value>
          <variable_type>ROOT_JOB</variable_type>
        </field>
        <field>
          <variable_name>app.hdfs.tables.dir</variable_name>
          <variable_value>&#x2f;Tables</variable_value>
          <variable_type>ROOT_JOB</variable_type>
        </field>
        <field>
          <variable_name>app.hdfs.ref.dir</variable_name>
          <variable_value>&#x2f;Reference</variable_value>
          <variable_type>ROOT_JOB</variable_type>
        </field>
        <field>
          <variable_name>app.hadoop.yarn.mem</variable_name>
          <variable_value>512</variable_value>
          <variable_type>ROOT_JOB</variable_type>
        </field>
        <field>
          <variable_name>app.hadoop.jvm.opts</variable_name>
          <variable_value>-Xmx256m</variable_value>
          <variable_type>ROOT_JOB</variable_type>
        </field>
        <field>
          <variable_name>app.hadoop.hdfs.replication</variable_name>
          <variable_value>1</variable_value>
          <variable_type>ROOT_JOB</variable_type>
        </field>
        <field>
          <variable_name>app.hadoop.hive.port</variable_name>
          <variable_value>10000</variable_value>
          <variable_type>ROOT_JOB</variable_type>
        </field>
        <field>
          <variable_name>app.hadoop.hive.user</variable_name>
          <variable_value>cskirde</variable_value>
          <variable_type>ROOT_JOB</variable_type>
        </field>
        <field>
          <variable_name>app.tower.tbl</variable_name>
          <variable_value>tower_table</variable_value>
          <variable_type>ROOT_JOB</variable_type>
        </field>
        <field>
          <variable_name>app.tower.tbl.parquet</variable_name>
          <variable_value>parquet_tower</variable_value>
          <variable_type>ROOT_JOB</variable_type>
        </field>
        <field>
          <variable_name>app.call.tbl</variable_name>
          <variable_value>call_table</variable_value>
          <variable_type>ROOT_JOB</variable_type>
        </field>
        <field>
          <variable_name>app.call.tbl.parquet</variable_name>
          <variable_value>parquet_call</variable_value>
          <variable_type>ROOT_JOB</variable_type>
        </field>
        <field>
          <variable_name>app.hdfs.tables.tower</variable_name>
          <variable_value>&#x2f;Tower</variable_value>
          <variable_type>ROOT_JOB</variable_type>
        </field>
        <field>
          <variable_name>app.hdfs.raw.tower.data</variable_name>
          <variable_value>&#x2f;TowerData</variable_value>
          <variable_type>ROOT_JOB</variable_type>
        </field>
        <field>
          <variable_name>app.hdfs.tables.call</variable_name>
          <variable_value>&#x2f;Call</variable_value>
          <variable_type>ROOT_JOB</variable_type>
        </field>
        <field>
          <variable_name>app.hdfs.raw.call.data</variable_name>
          <variable_value>&#x2f;CallData</variable_value>
          <variable_type>ROOT_JOB</variable_type>
        </field>
        <field>
          <variable_name>app.join.tbl</variable_name>
          <variable_value>join_table</variable_value>
          <variable_type>ROOT_JOB</variable_type>
        </field>
        <field>
          <variable_name>app.hdfs.tables.join</variable_name>
          <variable_value>&#x2f;Join</variable_value>
          <variable_type>ROOT_JOB</variable_type>
        </field>
      </fields>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>224</xloc>
      <yloc>64</yloc>
      </entry>
    <entry>
      <name>Delete folders</name>
      <description/>
      <type>DELETE_FOLDERS</type>
      <arg_from_previous>N</arg_from_previous>
      <success_condition>success_if_no_errors</success_condition>
      <limit_folders>10</limit_folders>
      <fields>
        <field>
          <name>hdfs&#x3a;&#x2f;&#x2f;&#x24;&#x7b;app.hadoop.hdfs.host&#x7d;&#x3a;&#x24;&#x7b;app.hadoop.hdfs.port&#x7d;&#x24;&#x7b;app.hdfs.base.dir&#x7d;&#x24;&#x7b;app.hdfs.tables.dir&#x7d;</name>
        </field>
      </fields>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>352</xloc>
      <yloc>64</yloc>
      </entry>
    <entry>
      <name>Hadoop Copy Files</name>
      <description/>
      <type>HadoopCopyFilesPlugin</type>
      <copy_empty_folders>Y</copy_empty_folders>
      <arg_from_previous>N</arg_from_previous>
      <overwrite_files>Y</overwrite_files>
      <include_subfolders>N</include_subfolders>
      <remove_source_files>N</remove_source_files>
      <add_result_filesname>N</add_result_filesname>
      <destination_is_a_file>N</destination_is_a_file>
      <create_destination_folder>Y</create_destination_folder>
      <fields>
        <field>
          <source_filefolder>hdfs&#x3a;&#x2f;&#x2f;localhost&#x3a;8020&#x24;&#x7b;app.hdfs.base.dir&#x7d;&#x24;&#x7b;app.hdfs.raw.tower.data&#x7d;</source_filefolder>
          <source_configuration_name>CDH 5.3</source_configuration_name>
          <destination_filefolder>hdfs&#x3a;&#x2f;&#x2f;localhost&#x3a;8020&#x24;&#x7b;app.hdfs.base.dir&#x7d;&#x24;&#x7b;app.hdfs.tables.dir&#x7d;&#x24;&#x7b;app.hdfs.tables.tower&#x7d;</destination_filefolder>
          <destination_configuration_name>CDH 5.3</destination_configuration_name>
          <wildcard/>
        </field>
        <field>
          <source_filefolder>hdfs&#x3a;&#x2f;&#x2f;localhost&#x3a;8020&#x24;&#x7b;app.hdfs.base.dir&#x7d;&#x2f;Ex2_Output</source_filefolder>
          <source_configuration_name>CDH 5.3</source_configuration_name>
          <destination_filefolder>hdfs&#x3a;&#x2f;&#x2f;localhost&#x3a;8020&#x24;&#x7b;app.hdfs.base.dir&#x7d;&#x24;&#x7b;app.hdfs.tables.dir&#x7d;&#x24;&#x7b;app.hdfs.tables.call&#x7d;</destination_filefolder>
          <destination_configuration_name>CDH 5.3</destination_configuration_name>
          <wildcard>&#x5e;&#x28;p&#x29;.&#x2a;</wildcard>
        </field>
      </fields>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>480</xloc>
      <yloc>64</yloc>
      </entry>
    <entry>
      <name>Populate Parquet Tables</name>
      <description/>
      <type>SQL</type>
      <sql/>
      <useVariableSubstitution>T</useVariableSubstitution>
      <sqlfromfile>F</sqlfromfile>
      <sqlfilename/>
      <sendOneStatement>F</sendOneStatement>
      <connection>ImpalaJoin</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>480</xloc>
      <yloc>240</yloc>
      </entry>
    <entry>
      <name>Abort job 2</name>
      <description/>
      <type>ABORT</type>
      <message/>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>128</xloc>
      <yloc>688</yloc>
      </entry>
    <entry>
      <name>Job</name>
      <description/>
      <type>JOB</type>
      <specification_method>filename</specification_method>
      <job_object_id/>
      <filename>&#x2f;home&#x2f;zeus&#x2f;Desktop&#x2f;WorkshopTraining&#x2f;08_big_data_joins&#x2f;Solution&#x2f;Ex4_BI_Job.kjb</filename>
      <jobname/>
      <arg_from_previous>N</arg_from_previous>
      <params_from_previous>N</params_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>N</set_logfile>
      <logfile/>
      <logext/>
      <add_date>N</add_date>
      <add_time>N</add_time>
      <loglevel>Nothing</loglevel>
      <slave_server_name/>
      <wait_until_finished>Y</wait_until_finished>
      <follow_abort_remote>N</follow_abort_remote>
      <expand_remote_job>N</expand_remote_job>
      <create_parent_folder>N</create_parent_folder>
      <pass_export>N</pass_export>
      <force_separate_logging>N</force_separate_logging>
      <parameters>        <pass_all_parameters>Y</pass_all_parameters>
      </parameters>      <set_append_logfile>N</set_append_logfile>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>720</xloc>
      <yloc>560</yloc>
      </entry>
  </entries>
  <hops>
    <hop>
      <from>Populate Tower Table</from>
      <to>Populate Call Data Table</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Populate Tower Table</from>
      <to>Abort job</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>N</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Populate Call Data Table</from>
      <to>Abort job</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>N</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Create Table for Joned Data Set</from>
      <to>Populate Joined Table</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Populate Joined Table</from>
      <to>Pentaho MapReduce</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>START</from>
      <to>Init Vars</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>Y</unconditional>
    </hop>
    <hop>
      <from>Init Vars</from>
      <to>Delete folders</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>Y</unconditional>
    </hop>
    <hop>
      <from>Delete folders</from>
      <to>Hadoop Copy Files</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Hadoop Copy Files</from>
      <to>Populate Tower Table</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Populate Call Data Table</from>
      <to>Populate Parquet Tables</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Populate Parquet Tables</from>
      <to>Create Table for Joned Data Set</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Populate Parquet Tables</from>
      <to>Abort job</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>N</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Create Table for Joned Data Set</from>
      <to>Abort job 2</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>N</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Populate Joined Table</from>
      <to>Abort job 2</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>N</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Pentaho MapReduce</from>
      <to>Abort job 2</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>N</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Pentaho MapReduce</from>
      <to>Job</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>Y</unconditional>
    </hop>
    <hop>
      <from>Job</from>
      <to>Success</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>Y</unconditional>
    </hop>
  </hops>
  <notepads>
    <notepad>
      <note>Initializing variables used in the job.&#xd;&#xa;Copying the input files for the exercise - in production we wouldn&#x27;t necessarily make copies of the file.&#xd;&#xa;</note>
      <xloc>608</xloc>
      <yloc>64</yloc>
      <width>628</width>
      <heigth>57</heigth>
      <fontname>Segoe UI</fontname>
      <fontsize>9</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>205</backgroundcolorgreen>
      <backgroundcolorblue>112</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
    <notepad>
      <note>Populating Hive tables based on our previous exercises.</note>
      <xloc>608</xloc>
      <yloc>240</yloc>
      <width>336</width>
      <heigth>26</heigth>
      <fontname/>
      <fontsize>-1</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>205</backgroundcolorgreen>
      <backgroundcolorblue>112</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
    <notepad>
      <note>Join our two large datasets&#xd;&#xa;Use Pentaho MapReduce to process the results of the join.</note>
      <xloc>624</xloc>
      <yloc>496</yloc>
      <width>353</width>
      <heigth>42</heigth>
      <fontname/>
      <fontsize>-1</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>205</backgroundcolorgreen>
      <backgroundcolorblue>112</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
  </notepads>
<attributes><group><name>JobRestart</name>
<attribute><key>UniqueConnections</key>
<value>N</value>
</attribute></group></attributes>

</job>
